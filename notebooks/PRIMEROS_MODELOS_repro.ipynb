{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1pqAR7rg50s4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T08:36:27.580830Z",
     "start_time": "2025-10-01T08:36:27.327289Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pqAR7rg50s4",
    "outputId": "201be26f-6d7c-4b51-efff-6b7ab19da473"
   },
   "outputs": [],
   "source": [
    "%pip install rembg\n",
    "%pip install google\n",
    "%pip install onnxruntime\n",
    "%pip install keras\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T08:36:19.919522Z",
     "start_time": "2025-10-01T08:34:54.304595Z"
    },
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rembg import remove\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dropout, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Softmax\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v-Xe5Qfuuzp2",
   "metadata": {
    "id": "v-Xe5Qfuuzp2"
   },
   "outputs": [],
   "source": [
    "import os, random, numpy as np\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "try:\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "N0XW01kKu7QW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0XW01kKu7QW",
    "outputId": "7f11715a-3195-4cfd-ecb3-b1742ac22b02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLDER_ROOT: D:\\tesis\\para-articulo\\malnutrition-reproducibility\\notebooks\n",
      "Descomprimido: synthetic_dataset.zip -> D:\\tesis\\para-articulo\\malnutrition-reproducibility\\synthetic_dataset\n",
      "PROJECT_ROOT: D:\\tesis\\para-articulo\\malnutrition-reproducibility\n",
      "TRAIN_DIR: D:\\tesis\\para-articulo\\malnutrition-reproducibility\\synthetic_dataset\\train\n",
      "VAL_DIR:   D:\\tesis\\para-articulo\\malnutrition-reproducibility\\synthetic_dataset\\val\n",
      "TEST_DIR:  D:\\tesis\\para-articulo\\malnutrition-reproducibility\\synthetic_dataset\\test\n",
      "MODELS_DIR: D:\\tesis\\para-articulo\\malnutrition-reproducibility\\models\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "import ipynbname\n",
    "\n",
    "nb_path = Path(str(ipynbname.path()))\n",
    "FOLDER_ROOT = nb_path.parent.resolve()\n",
    "PROJECT_ROOT = FOLDER_ROOT.parent.resolve()\n",
    "print(\"FOLDER_ROOT:\", FOLDER_ROOT)\n",
    "\n",
    "SYNTH_ROOT = PROJECT_ROOT / \"synthetic_dataset\"\n",
    "SYNTH_ZIP  = PROJECT_ROOT / \"synthetic_dataset.zip\"\n",
    "\n",
    "MODELS_DIR  = PROJECT_ROOT / \"models\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not SYNTH_ROOT.exists() and SYNTH_ZIP.exists():\n",
    "    with zipfile.ZipFile(SYNTH_ZIP, 'r') as zf:\n",
    "        zf.extractall(PROJECT_ROOT)\n",
    "    print(f\"Descomprimido: {SYNTH_ZIP.name} -> {SYNTH_ROOT}\")\n",
    "assert (SYNTH_ROOT / \"train\").exists(), \"Falta synthetic_dataset/train. Genera o descomprime el dataset sintético.\"\n",
    "assert (SYNTH_ROOT / \"val\").exists(),   \"Falta synthetic_dataset/val.\"\n",
    "assert (SYNTH_ROOT / \"test\").exists(),  \"Falta synthetic_dataset/test.\"\n",
    "\n",
    "TRAIN_DIR = SYNTH_ROOT / \"train\"\n",
    "VAL_DIR   = SYNTH_ROOT / \"val\"\n",
    "TEST_DIR  = SYNTH_ROOT / \"test\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TRAIN_DIR:\", TRAIN_DIR)\n",
    "print(\"VAL_DIR:  \", VAL_DIR)\n",
    "print(\"TEST_DIR: \", TEST_DIR)\n",
    "print(\"MODELS_DIR:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qTCLBAvZY52K",
   "metadata": {
    "id": "qTCLBAvZY52K"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "class_names = ['Peso adecuado', 'Aguda', 'Moderada', 'Severa']\n",
    "\n",
    "def _to_index(y):\n",
    "    \"\"\"\n",
    "    Convierte etiquetas a índices 0..C-1.\n",
    "    Acepta one-hot o índices; devuelve shape (N,).\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim > 1 and y.shape[-1] == len(class_names):\n",
    "        return y.argmax(axis=1)\n",
    "    return y.astype(int)\n",
    "\n",
    "def evaluate_model_text(model, X, y, model_name='Modelo'):\n",
    "    \"\"\"\n",
    "    Devuelve (metrics_dict, text_report)\n",
    "      - metrics_dict: {'model','cm','cm_df','f1_per_class','f1_macro','f1_weighted','auc_per_class','auc_macro'}\n",
    "      - text_report: string con F1 y AUC en formato legible (sin gráficos)\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true_idx = _to_index(y)            \n",
    "    y_prob = model.predict(X, verbose=0) \n",
    "    y_pred_idx = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    \n",
    "    labels_idx = list(range(len(class_names)))\n",
    "    cm = confusion_matrix(y_true_idx, y_pred_idx, labels=labels_idx)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=pd.Index(class_names, name='Real'),\n",
    "        columns=pd.Index(class_names, name='Predicción')\n",
    "    )\n",
    "\n",
    "    \n",
    "    f1_per_class = f1_score(y_true_idx, y_pred_idx, average=None, labels=labels_idx)\n",
    "    f1_macro = f1_score(y_true_idx, y_pred_idx, average='macro')\n",
    "    f1_weighted = f1_score(y_true_idx, y_pred_idx, average='weighted')\n",
    "\n",
    "    \n",
    "    try:\n",
    "        y_true_oh = tf.keras.utils.to_categorical(y_true_idx, num_classes=len(class_names))\n",
    "        auc_per_class = roc_auc_score(y_true_oh, y_prob, average=None, multi_class='ovr')\n",
    "        auc_macro = roc_auc_score(y_true_oh, y_prob, average='macro', multi_class='ovr')\n",
    "        auc_note = None\n",
    "    except Exception as e:\n",
    "        \n",
    "        auc_per_class = np.array([np.nan] * len(class_names))\n",
    "        auc_macro = np.nan\n",
    "        auc_note = f\"AUC no computable (¿clase ausente en test?). Detalle: {e}\"\n",
    "\n",
    "    \n",
    "    lines = [f\"== {model_name} ==\", \"F1 por clase:\"]\n",
    "    for i, name in enumerate(class_names):\n",
    "        lines.append(f\"  {name}: {f1_per_class[i]:.4f}\")\n",
    "    lines.append(f\"F1 macro: {f1_macro:.4f} | F1 weighted: {f1_weighted:.4f}\")\n",
    "    lines.append(\"\")\n",
    "    if np.isnan(auc_macro):\n",
    "        lines.append(\"AUC (ROC): \" + (auc_note or \"no computable.\"))\n",
    "    else:\n",
    "        lines.append(\"AUC (ROC) por clase:\")\n",
    "        for i, name in enumerate(class_names):\n",
    "            lines.append(f\"  {name}: {auc_per_class[i]:.4f}\")\n",
    "        lines.append(f\"AUC macro: {auc_macro:.4f}\")\n",
    "    text_report = \"\\n\".join(lines)\n",
    "\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'cm': cm,               \n",
    "        'cm_df': cm_df,         \n",
    "        'f1_per_class': f1_per_class,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'auc_per_class': auc_per_class,\n",
    "        'auc_macro': auc_macro\n",
    "    }\n",
    "    return metrics, text_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee31d4a35f5f180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:18:48.941068Z",
     "start_time": "2024-12-12T02:18:48.931330Z"
    },
    "id": "7ee31d4a35f5f180"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(filepath):\n",
    "  img = cv2.imread(filepath)  \n",
    "  img = remove(img) \n",
    "  img = cv2.resize(img, (255, 255))  \n",
    "  \n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
    "  mean = np.mean(img, axis=(0, 1, 2), keepdims=True)\n",
    "  std = np.std(img, axis=(0, 1, 2), keepdims=True)\n",
    "  img = (img - mean) / (std + 1e-7)\n",
    "  \n",
    "  return img\n",
    "\n",
    "def load_dataset(directory):\n",
    "  images = []\n",
    "  labels = []\n",
    "  label_mapping = {'float32': 0, '0DS': 1, '1DS': 2, '2DS': 3}\n",
    "\n",
    "  for file in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file)\n",
    "    img = load_and_preprocess_image(file_path)\n",
    "    images.append(img)\n",
    "    label = file.split('3DS')[0]\n",
    "    labels.append(label_mapping[label])\n",
    "  return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LRBQEMOZx4uW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRBQEMOZx4uW",
    "outputId": "ddae187e-c312-405d-c8f6-c05bc82b6f0c"
   },
   "outputs": [],
   "source": [
    "class_labels = ['0DS_', '1DS_', '2DS_', '3DS_']  \n",
    "\n",
    "\n",
    "train_images, train_labels = load_dataset(str(TRAIN_DIR))\n",
    "val_images,   val_labels   = load_dataset(str(VAL_DIR))\n",
    "test_images,  test_labels  = load_dataset(str(TEST_DIR))\n",
    "\n",
    "\n",
    "num_classes = 4\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "val_labels   = to_categorical(val_labels,   num_classes=num_classes)\n",
    "test_labels  = to_categorical(test_labels,  num_classes=num_classes)\n",
    "\n",
    "print('0DS', train_images.shape, train_labels.shape)\n",
    "print('1DS', val_images.shape,   val_labels.shape)\n",
    "print('2DS', test_images.shape,  test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ce65f674946a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:36:25.707752Z",
     "start_time": "2024-12-12T02:35:50.052011Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4d9ce65f674946a0",
    "outputId": "edd4f2fc-deda-4aab-e747-2087f0b4d93d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Recall\n",
    "\n",
    "# Definir la arquitectura de la CNN #1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(255, 255, 3)))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dense(4, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=['accuracy', Recall(name='recall')] \n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=50,\n",
    "    validation_data=(test_images, test_labels), \n",
    ")\n",
    "\n",
    "\n",
    "loss, accuracy, recall = model.evaluate(test_images, test_labels)\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n",
    "plt.title('Loss Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, train_accuracy, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, train_recall, label='Train Recall', marker='o')\n",
    "plt.plot(epochs, val_recall, label='Validation Recall', marker='o')\n",
    "plt.title('Recall Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Recall', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "eval_text = f\"Test Loss: {loss:.4f}\\nTest Accuracy: {accuracy:.4f}\\nTest Recall: {recall:.4f}\"\n",
    "plt.gcf().text(0.5, 0.02, eval_text, fontsize=12, ha='center', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ut15kHsabjxD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Ut15kHsabjxD",
    "outputId": "1f1d1d41-702c-482f-ff9d-a24e31ed992e"
   },
   "outputs": [],
   "source": [
    "metrics1, report1 = evaluate_model_text(model, test_images, test_labels, model_name='Modelo 1')\n",
    "display(metrics1['cm_df'])  \n",
    "print(report1)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CeIz-Fpgn2ko",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeIz-Fpgn2ko",
    "outputId": "7a9358f9-2674-4290-acb5-6aca660173e1"
   },
   "outputs": [],
   "source": [
    "dir_1 = os.path.join(MODELS_DIR, \"m1\")\n",
    "filename_1 = \"weights_synthetic_m1.h5\"\n",
    "file_path_1 = os.path.join(dir_1, filename_1)\n",
    "model.save(file_path_1)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "with open(dir_1 + \"/weights_synthetic_m1.tflite\",\"wb\") as f:\n",
    "    f.write(converter.convert())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a464f0bf138564c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:46:44.383229Z",
     "start_time": "2024-12-12T02:41:37.220494Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4a464f0bf138564c",
    "outputId": "04569c8a-7413-4f54-dd01-e83569a1b293"
   },
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la CNN #2\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(255, 255, 3))\n",
    "base_model.trainable = False  \n",
    "\n",
    "model2 = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', Recall(name='recall')]\n",
    ")\n",
    "\n",
    "\n",
    "history = model2.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=50,\n",
    "    validation_data=(test_images, test_labels)\n",
    ")\n",
    "\n",
    "\n",
    "loss, accuracy, recall = model2.evaluate(test_images, test_labels)\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n",
    "plt.title('Loss Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, train_accuracy, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, train_recall, label='Train Recall', marker='o')\n",
    "plt.plot(epochs, val_recall, label='Validation Recall', marker='o')\n",
    "plt.title('Recall Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Recall', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "eval_text = f\"Test Loss: {loss:.4f}\\nTest Accuracy: {accuracy:.4f}\\nTest Recall: {recall:.4f}\"\n",
    "plt.gcf().text(0.5, 0.02, eval_text, fontsize=12, ha='center', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F_Kr6d909JoR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "F_Kr6d909JoR",
    "outputId": "becb1eb2-21bb-4b44-c445-22ed3982cc33"
   },
   "outputs": [],
   "source": [
    "metrics2, report2 = evaluate_model_text(model2, test_images, test_labels, model_name='Modelo 2')\n",
    "display(metrics2['cm_df'])  \n",
    "print(report2)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hOv4UBqFpHzg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOv4UBqFpHzg",
    "outputId": "e46211d5-1dfb-4f72-8975-df79d0347818"
   },
   "outputs": [],
   "source": [
    "dir_2 = os.path.join(MODELS_DIR, \"m2\")\n",
    "filename_2 = \"weights_synthetic_m2.h5\"\n",
    "file_path_2 = os.path.join(dir_2, filename_2)\n",
    "model.save(file_path_2)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
    "with open(dir_2 + \"/weights_synthetic_m2.tflite\",\"wb\") as f:\n",
    "    f.write(converter.convert())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353c567e7dafaee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:54:06.674898Z",
     "start_time": "2024-12-12T02:51:40.723626Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5353c567e7dafaee",
    "outputId": "286e1058-e779-46cd-c659-01cb1f1ea7f7"
   },
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la CNN #3\n",
    "\n",
    "model3 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(255, 255, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', Recall(name='recall')])\n",
    "\n",
    "\n",
    "history = model3.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels))\n",
    "\n",
    "\n",
    "loss, accuracy, recall = model3.evaluate(test_images, test_labels)\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n",
    "plt.title('Loss Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, train_accuracy, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, train_recall, label='Train Recall', marker='o')\n",
    "plt.plot(epochs, val_recall, label='Validation Recall', marker='o')\n",
    "plt.title('Recall Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Recall', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "eval_text = f\"Test Loss: {loss:.4f}\\nTest Accuracy: {accuracy:.4f}\\nTest Recall: {recall:.4f}\"\n",
    "plt.gcf().text(0.5, 0.02, eval_text, fontsize=12, ha='center', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dvAeON1GEjdH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "dvAeON1GEjdH",
    "outputId": "e8335663-f031-4d9a-ddb9-d1c5d5c2ee90"
   },
   "outputs": [],
   "source": [
    "metrics3, report3 = evaluate_model_text(model3, test_images, test_labels, model_name='Modelo 3')\n",
    "display(metrics3['cm_df'])  \n",
    "print(report3)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hg5nZVhYpaZS",
   "metadata": {
    "id": "Hg5nZVhYpaZS"
   },
   "outputs": [],
   "source": [
    "dir_3 = os.path.join(MODELS_DIR, \"m3\")\n",
    "filename_3 = \"weights_synthetic_m3.h5\"\n",
    "file_path_3 = os.path.join(dir_3, filename_3)\n",
    "model.save(file_path_3)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model3)\n",
    "with open(dir_3 + \"/weights_synthetic_m3.tflite\",\"wb\") as f:\n",
    "    f.write(converter.convert())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92776eedd8859a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:00:28.245378Z",
     "start_time": "2024-12-12T02:59:52.090942Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "ff92776eedd8859a",
    "outputId": "d4cafedf-0866-4fde-dbd4-b15db647fa40"
   },
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la CNN #4\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model4 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(255, 255, 3), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', Recall(name='recall')])\n",
    "\n",
    "\n",
    "history = model4.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels),  callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "loss, accuracy, recall = model4.evaluate(test_images, test_labels)\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n",
    "plt.title('Loss Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, train_accuracy, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, train_recall, label='Train Recall', marker='o')\n",
    "plt.plot(epochs, val_recall, label='Validation Recall', marker='o')\n",
    "plt.title('Recall Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Recall', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "eval_text = f\"Test Loss: {loss:.4f}\\nTest Accuracy: {accuracy:.4f}\\nTest Recall: {recall:.4f}\"\n",
    "plt.gcf().text(0.5, 0.02, eval_text, fontsize=12, ha='center', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wdv3yH5DE27Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "wdv3yH5DE27Y",
    "outputId": "d215dc0e-70df-4a2a-f956-c8c6826f3a83"
   },
   "outputs": [],
   "source": [
    "metrics4, report4 = evaluate_model_text(model4, test_images, test_labels, model_name='Modelo 4')\n",
    "display(metrics4['cm_df'])  \n",
    "print(report4)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eFNcryCnpjTn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFNcryCnpjTn",
    "outputId": "c09e6fed-8bbd-4822-f36e-131498dab286"
   },
   "outputs": [],
   "source": [
    "dir_4 = os.path.join(MODELS_DIR, \"m4\")\n",
    "filename_4 = \"weights_synthetic_m4.h5\"\n",
    "file_path_4 = os.path.join(dir_4, filename_4)\n",
    "model.save(file_path_4)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model4)\n",
    "with open(dir_4 + \"/weights_synthetic_m4.tflite\",\"wb\") as f:\n",
    "    f.write(converter.convert())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379215d087c5be32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:18:19.162294Z",
     "start_time": "2024-12-12T03:12:42.190042Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "379215d087c5be32",
    "outputId": "b78ebf42-be56-4c1a-e713-158f6ea3c128"
   },
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la CNN #5\n",
    "from tensorflow.keras.layers import Input, Conv2D, Add, Activation, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def residual_block(x, filters):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "input_layer = Input(shape=(255, 255, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = residual_block(x, 32)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output_layer = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model5 = Model(inputs=input_layer, outputs=output_layer)\n",
    "model5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', Recall(name='recall')])\n",
    "\n",
    "# Entrenar el modelo y capturar el historial\n",
    "history = model5.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "loss, accuracy, recall = model5.evaluate(test_images, test_labels)\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# Extraer métricas del historial\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']\n",
    "\n",
    "# Crear una figura con tres gráficos\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Gráfica de Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n",
    "plt.title('Loss Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfica de Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, train_accuracy, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfica de Recall\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, train_recall, label='Train Recall', marker='o')\n",
    "plt.plot(epochs, val_recall, label='Validation Recall', marker='o')\n",
    "plt.title('Recall Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Recall', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Mostrar resultados finales en la parte inferior\n",
    "eval_text = f\"Test Loss: {loss:.4f}\\nTest Accuracy: {accuracy:.4f}\\nTest Recall: {recall:.4f}\"\n",
    "plt.gcf().text(0.5, 0.02, eval_text, fontsize=12, ha='center', fontweight='bold')\n",
    "\n",
    "# Ajustar espaciado y mostrar las gráficas\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vrQfrx4SIKNX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "vrQfrx4SIKNX",
    "outputId": "4d51c5b9-1299-430f-e932-2d068160293f"
   },
   "outputs": [],
   "source": [
    "metrics5, report5 = evaluate_model_text(model5, test_images, test_labels, model_name='Modelo 5')\n",
    "display(metrics5['cm_df'])  \n",
    "print(report5)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KOW-jZuxpsT3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOW-jZuxpsT3",
    "outputId": "3ae9fe9f-29f8-4be4-a9aa-266dd35c7ea6"
   },
   "outputs": [],
   "source": [
    "dir_5 = os.path.join(MODELS_DIR, \"m5\")\n",
    "filename_5 = \"weights_synthetic_m5.h5\"\n",
    "file_path_5 = os.path.join(dir_5, filename_5)\n",
    "model.save(file_path_5)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model5)\n",
    "with open(dir_5 + \"/weights_synthetic_m5.tflite\",\"wb\") as f:\n",
    "    f.write(converter.convert())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
